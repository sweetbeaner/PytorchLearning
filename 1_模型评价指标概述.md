# 模型评价指标概述
利用【指标评价模型】对模型进行评价
学习常用的评价指标
比较不同指标之间的适应性和优缺点

## 1.混淆矩阵（误差矩阵）
误差矩阵是一种评价模型精度的标准格式，用n * n的矩阵进行表示

列代表预测的类别，行代表实际的类别

通过矩阵，对分类模型的预测结果与真实标签之间的关系进行可视化表示

混淆矩阵的组成：

+ 真正列（True Positives，TP）：模型正确预测为正例的样本数量————正确预测了“是”的数量
+ 真反例（True Negatives，TN）：模型正确预测为反例的样本数量————正确预测了“非”的数量
+ 假正例（False Positives，FP）：模型错误预测为正例的样本数量————错误预测了“是”的数量
+ 假反例（False Negatives，FN）：模型错误预测为负例的样本数量————错误预测了“非”的数量

混淆矩阵|预测为正例|预测为负例
---|:--:|---:
实际为正例|TP|FP
实际为负例|FN|TN


基于这些指标，可以计算出一些常用的分类性能指标，如准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1 分数等。这些指标可以帮助我们更全面地评估分类模型的性能，并了解模型在不同类别上的表现。


## 2.总体准确率（Overall Accuracy，OA）
用于评估分类模型性能的指标，它衡量模型在所有样本上正确分类的比例。

代表了所有预测正确的样本（TP+TN）占所有预测样本总数（TP+TN+FP+FN）的比例。

OA = （TP+TN）/（TP+TN+FP+FN）= Ncorrect/Ntotal

## 3.平均准确率（Average accuracy，AA）
平均精度计算的是每一类预测正确的样本与该类总体数量之间的比值，最终再取每一类的精度的平均值。

代表了将正例样本的预测准确率和反例样本的预测准确率的平均值。

代码实现中，我们使用numpy的diag将混淆矩阵的对角线元素取出，并且对于混淆矩阵进行列求和，用对角线元素除以求和后的结果，最后对结果计算求出平均值。

AA = （TP/（TP+FN）+（TN/(FP+TN)）/ 2

## 4.Kappa系数（Kappa coefficient）
一个用于一致性检验的指标，也可以用于衡量分类的效果。

用于度量分类器或评估者之间一致性的统计指标。

它衡量了观察到的一致性与随机一致性之间的差异。

通常用于评估分类任务中的分类器性能，特别是在面对类别不平衡或者评估者主观判断时，它比简单的准确率更具有信息量。

Kappa系数的取值范围从-1到1，具体含义如下：
+ Kappa系数为1时，表示完全一致。
+ Kappa系数为0时，表示观察到的一致性与随机一致性之间没有差异。
+ Kappa系数小于0时，表示观察到的一致性低于随机一致性。

kappa =  (准确率 - 随机一致性) / (1 - 随机一致性)

## 5.召回率（Recall）
正样本的识别率。

实际为正样本并且也被正确识别为正样本的数量占样本中所有为正样本的比例。

代表了模型对于正样本的识别能力，也能较好的反应模型的优劣。

Recall = TP/(TP+FN)

## 6.精准率（Precision）
全部预测为正例的结果中，正确的比例

代表了模型对于正样本的识别能力。

与Recall的区别是，Precision预测结果中有多少样本是分类正确的。

结合核酸的例子，当我们在进行全面核酸检测时，我们的希望在于模型尽可能少的漏掉阳性患者，此时认为模型Precision显得更为重要。

Precision = TP/(TP+FP)

## 7.F1
F1可以解释为召回率（Recall）和P（精确率）的加权平均，F1越高，说明模型鲁棒性越好。

### 7.1 F-score
广义F1，改变P和R的权重
Fβ = （（1+β^2）* P * R）/（β^2 * P）+R

+ 当β>1时，更偏好召回率（Recall）
+ 当β<1时，更偏好精准（Precision）
+ 当β=1时，平衡精准和召回率（F1）

当有多个混淆矩阵（多次训练、多个数据集、多分类任务）时，有两种方式估算 “全局” 性能：

+ macro方法：先计算每个PR，取平均后，再计算 F1
+ micro方法：先计算混淆矩阵元素的平均，再计算 PR 和 F1

## 8.PR曲线
PR曲线也能很直观的反应模型好坏。

横轴是召回率，纵轴代表了P（精确率)，P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。

PR曲线可以帮助我们综合考虑精确率P和召回率R之间的权衡。

曲线上的每个点表示在不同预测阈值下的模型性能。

PR曲线越靠近右上角，表示模型具有更好的性能，同时具有较高的精确率和召回率。

如果PR曲线下的面积较大，表示模型在各个阈值下的整体性能较好。

## 9.置信度
对一个估计值或者模型预测的信心水平。它表示对结果的可信程度或者可靠性的度量。

置信度的值通常介于0和1之间，可以看作是一个概率值，表示我们对结果的相对信任程度。

在目标检测中划分正负样本，当置信度小于阈值判定为负样本，大于则判定为正样本。

+ 置信区间（Confidence Interval）：在统计推断中，置信区间是对未知参数的估计结果给出的范围，以一定的置信度表示。例如，95%置信区间表示我们有95%的信心认为真实参数值落在该区间内。
+ 分类器预测的置信度（Classifier Confidence）：在分类问题中，一些分类模型（如支持向量机、神经网络等）可以给出样本属于某个类别的置信度或概率。这个置信度值可以用于评估模型对预测结果的信心程度，以及进行后续的决策或阈值设置。
+ 置信度评分（Confidence Score）：在一些机器学习任务中，模型可以为每个样本生成一个置信度评分。该评分可以表示模型对该样本预测的置信程度。较高的置信度评分通常表示模型对其预测结果有较高的信心。

## 10.交并比（Intersection over Union，IOU）
评估目标检测算法性能的指标，用于衡量预测边界框与真实边界框之间的重叠程度。

通过计算预测边界框与真实边界框之间的交集面积与并集面积之比来度量它们的重叠程度。

IOU = Area of Intersection / Area of Union

IOU 的取值范围在 0 到 1 之间：
+ 当 IOU 为 0 时，表示预测边界框与真实边界框没有重叠区域。
+ 当 IOU 为 1 时，表示预测边界框与真实边界框完全重合，即完全准确的预测。

在实际模型识别时会根据我们自己设定合适的阈值来判定正负样本，一般设置为0.7.

## 11. 平均精确率（Average Precision，AP）
AP 是一种常用的评估信息检索任务（如目标检测、图像检索等）中算法性能的指标，用于衡量检索结果的准确性和排序质量。

在信息检索任务中，通常会根据算法的预测结果对相关性进行排序，并根据排序结果计算精确率（Precision）和召回率（Recall）。

然后，根据不同召回率下的精确率计算出 Average Precision。

AP 的计算方法如下：
+ 计算不同召回率下的精确率。从高到低按照预测结果的置信度或分数对结果进行排序。
+ 计算每个召回率点处的精确率。在每个召回率点上，计算该点之前（包括该点）的最大精确率值。
+ 将各个召回率点处的最大精确率值求平均，得到 Average Precision。

AP 的取值范围在 0 到 1 之间：
+ 当所有召回率点处的精确率为 0 时，AP 为 0。
+ 当所有召回率点处的精确率都为 1 时，AP 为 1。

AP 可以提供一种**综合考虑检索结果准确性和排序质量**的度量（综合指标）。

较高的 AP 值通常表示算法在返回的结果中包含更多相关的项，并且这些相关项排名靠前。

## 12.mean Average Precision（mAP）
所有类的AP值的平均值，综合考虑**不同查询或类别的性能表现**（更加综合的指标）。

mAP 的取值范围也是从 0 到 1，与 AP 相同：
+ 当所有查询或类别的 AP 值都为 0 时，mAP 为 0。
+ 当所有查询或类别的 AP 值都为 1 时，mAP 为 1。

mAP 是一种常用的评估指标，特别适用于多类别目标检测、图像检索和信息检索等任务。

它提供了对**整体性能的评估**，能够综合考虑**不同查询或类别**的**性能差异**，并更**全面地衡量算法的效果**。







