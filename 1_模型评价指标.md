# 模型评价指标
利用【指标评价模型】对模型进行评价
学习常用的评价指标
比较不同指标之间的适应性和优缺点

## 1.混淆矩阵（误差矩阵）
误差矩阵是一种评价模型精度的标准格式，用n * n的矩阵进行表示

列代表预测的类别，行代表实际的类别

通过矩阵，对分类模型的预测结果与真实标签之间的关系进行可视化表示

混淆矩阵的组成：

+ 真正列（True Positives，TP）：模型正确预测为正例的样本数量————正确预测了“是”的数量
+ 真反例（True Negatives，TN）：模型正确预测为反例的样本数量————正确预测了“非”的数量
+ 假正例（False Positives，FP）：模型错误预测为正例的样本数量————错误预测了“是”的数量
+ 假反例（False Negatives，FN）：模型错误预测为负例的样本数量————错误预测了“非”的数量



基于这些指标，可以计算出一些常用的分类性能指标，如准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1 分数等。这些指标可以帮助我们更全面地评估分类模型的性能，并了解模型在不同类别上的表现。


## 2.总体准确率（Overall Accuracy，OA）
用于评估分类模型性能的指标，它衡量模型在所有样本上正确分类的比例。

代表了所有预测正确的样本（TP+TN）占所有预测样本总数（TP+TN+FP+FN）的比例。

OA = （TP+TN）/（TP+TN+FP+FN）= Ncorrect/Ntotal

## 3.平均准确率（Average accuracy，AA）
平均精度计算的是每一类预测正确的样本与该类总体数量之间的比值，最终再取每一类的精度的平均值。

代表了将正例样本的预测准确率和反例样本的预测准确率的平均值。

代码实现中，我们使用numpy的diag将混淆矩阵的对角线元素取出，并且对于混淆矩阵进行列求和，用对角线元素除以求和后的结果，最后对结果计算求出平均值。

AA = （TP/（TP+FP）+（TN/(FP+FN)）/ 2

## 4.Kappa系数（Kappa coefficient）
一个用于一致性检验的指标，也可以用于衡量分类的效果。

用于度量分类器或评估者之间一致性的统计指标。

它衡量了观察到的一致性与随机一致性之间的差异。

通常用于评估分类任务中的分类器性能，特别是在面对类别不平衡或者评估者主观判断时，它比简单的准确率更具有信息量。

Kappa系数的取值范围从-1到1，具体含义如下：
+ Kappa系数为1时，表示完全一致。
+ Kappa系数为0时，表示观察到的一致性与随机一致性之间没有差异。
+ Kappa系数小于0时，表示观察到的一致性低于随机一致性。

kappa =  (准确率 - 随机一致性) / (1 - 随机一致性)
